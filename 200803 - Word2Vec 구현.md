# Chapter3 word2vec


## Matmul 클래스

```python
class Matmul:
  def __init__(self,W):
    self.params=[W]
    self.grads=[np.zeros_like(W)]
    self.x=None
    
  def forward(self,x):
    W,=self.params
    out=np.matmul(x,W)
    self.x=x
    return out
    
  def backward(self,dout):
    W,=self.params
    dx=np.matmul(dout,W.T)
    dW=np.matmul(self.x.T,dout)
    self.grads[0][...]=dW
    return dx 
```

## 전처리 함수 
```python
def preprocess(text):
  text=text.lower()
  text=text.replace('.',' .')
  words=text.split(' ')
  word_to_id={}
  a=0
  for i in words:
    if i not in word_to_id:
      word_to_id[i]=a
      a+=1
  id_to_word={}
  a=0
  for i in words:
    if i not in list(id_to_word.values()):
    id_to_word[a]=i
    a+=1
  corpus=[]
    for i in words:
      corpus.append(word_to_id[i])
  return corpus, word_to_id, id_to_word 
```

## 전처리 후 one_hot_vector 변환, contexts, target 변환
```python
def create_contexts_target(corpus,window_size):
  contexts=[]
  target=[]
  for i in range(len(corpus)):
    a=[0]*(len(corpus)-1)
    a[corpus[i]]=1
    corpus=list(corpus)
    corpus[i]=a
  for i in range(1,len(corpus)-window_size):
    contexts.append([corpus[i-window_size],corpus[i+window_size]])
    target.append(corpus[i])
  return np.array(contexts),np.array(target)  
```

## CBOW 클래스 구현

- CBOW --> 맥락으로부터 타깃을 추론
- 단어 벡터 --> 입력층의 가중치(W_in) 로 표현
- 활성함수 --> 다중 클래스 분류: 소프트맥스 함수
- 손실 함수 --> 교차 엔트로피 오차

```python
 class simpleCBOW:
    def __init__(self,vocab_size,hidden_size):
      v,h=vocab_size,hidden_size
      w_in=0.01*np.random.randn(v,h).astype('f')
      w_out=0.01*np.random.randn(h,v).astype('f')
      self.in_layer0=Matmul(w_in)
      self.in_layer1=Matmul(w_in)
      self.out_layer=Matmul(w_out)
      self.loss_layer=SoftmaxWithLoss()
      layers=[self.in_layer0, self.in_layer1, self.out_layer]
      self.params, self.grads=[], []
      
      for layer in layers:
        self.params+=layer.params
        self.grads+=layer.grads
         self.word_vecs=w_in
         
     def forward(self,contexts,target):
        h0=self.in_layer0.forward(contexts[:,0])
        h1=self.in_layer1.forward(contexts[:,1])
        h=0.5*(h0+h1)
        score=self.out_layer.forward(h)
        loss=self.loss_layer.forward(score,target)
        return loss
        
     def backward(self,dout=1):
        ds=self.loss_layer.backward(dout)
        da=self.out_layer.backward(ds)
        da*=0.5
        self.in_layer0.backward(da)
        self.in_layer1.backward(da)
        return None
```

## simple CBOW 학습 구현

```python
text='You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
hidden_size = 5
window_size = 1
max_epoch = 100
batch_size =3
contexts, target = create_contexts_target(corpus,window_size)
model = simpleCBOW(vocab_size,hidden_size)
from trainer import Trainer
from optimizer import Adam
optimizer= Adam()
trainer = Trainer(model,optimizer)
trainer.fit(contexts,target,max_epoch, batch_size)
trainer.plot()
```

## Embedding 계층
- 입력층의 원핫 표현과 가중치 행렬 W_in 곱의 계산 복잡성 해결을 위함
- 단어 ID에 해당하는 행(벡터)을 추출하는 계층 --> 계산 복잡성 해결
- Backward 계산시, idx가 중복되는 경우 마지막 경우로 덮어쓰기 되므로 np.add.at 이용해 중복 문제를 '더하기' 로 해결

```python

class Embedding:
  def __init__(self,w):
      self.params=[w]
      self.grads=[np.zeros_like(w)]
      self.idx=None
  
  def forward(self,idx):
      w,=self.params
      self.idx=idx
      out=w[idx]
      return out
  
  def backward(self,dout):
      dw,=[self.grads]
      np.add.at(dw,self.idx,dout)
      return None
```

## W_OUT 부분 Emebedding

- '다중 분류' 에서 '이진 분류' 문제로 변환
- Embedding 계층의 foward(idx) 로 taget 을 구함
- 이후 W_in Embedding 으로 넘어온 h 와 내적 계산

```python
  class EmbeddingDot:
      def __init__(self,w):
          self.embed=Embedding(w)
          self.params=self.embed.params
          self.grads=self.embed.grads
          self.cache = None
      
      def forward(self,h,idx):
          target_w = self.embed.forward(idx)
          out=np.sum(target_w*h,axis=1)
          self.cache=(h,target_w)
          return out

      def backward(self,dout):
          h,target_w=sel.cache
          dtaget_w=dout*h
          dh=dout*dtarget_w
          self.embed.backward(dtarget_w)
          return dh

          ## W_OUT 부분 Emebedding        
          
```


## Negative Sampling 구현

- 긍정적인 예(정답 레이블) 뿐만 아니라 부정적인 예(오답 레이블)에 대해서도 학습 해야 하므로 해당 계층 필요
- 모든 오답 레이블을 처리할 수 없으므로 자주 등장하는 오답 단어를 추출하여 손실을 구함
- 각 레이블의 모든 손실을 더하여 최종 손실을 구한다.

```python

import sys
sys.path.append('C:/Users/HYS/NLP/ch04')
from negative_sampling_layer import UnigramSampler

class NegativeSamplingLoss:
    def __init__(self,w,corpus,power=0.75,sample_size=5):
        self.sample_size=sample_size
        self.sampler=UnigramSample(corpus,power,sample_size)
        self.loss_layers=[SigmoidWithLoss() for _ in range(sample_size+1)]
        self.embed_dot_layers=[EmbeddingDot(w) for _ in range(sample_size+1)]
        self.params, self.grads = [], []
        
        for layer in self.embed_dot_layers:
            self.params+=layer.params
            self.grads+=layer.grads
    
    def forward(self,h,target):
        batch_size = target_shape[0]
        negative_sample = self.sampler.get_negative_sample(target)
        #긍정
        score=self.embed_dot_layers[0].forward(h,target)
        correct_label=np.ones(batch_size,dtype=np.int32)
        loss=self.loss_layers[0].forward(score,correct_label)
        #부정
        negative_label=np.zeros(batch_size, dtype=np.int32)
        for i in range(self.sample_size):
            negative_target=negative_sample[:,i]
            score = self.embed_dot_layers[i+1].forward(h,negative_target)
            loss += self.loss_layers[i+1].forward(score,negatvie_label)
        return loss

      def backward(self, dout=1):
          h=0
          for i,j in zip(self.loss_layers, self.embed_layers):
          dscore=i.backward(dout)
          dh+=j.backward(dscore)
          return dh
   
 ```
 
## 개선된 CBOW 구현
```python
    class CBOW2:
        def __init__(self,vocab_size,hidden_size,window_size,corpus):
              v,h = vocab_size, hidden_size
              
              # weight 초기화
              w_in = np.random.randn(v,h).astype('f')
              w_out = np.random.randn(h,v).astype('f')
              
              # in 계층 생성
              self.in_layers=[]
              for i in range(2*window_size):
                  self.in_layers.append(Embedding(w_in))
               # loss 계층 생성
              self.ns_loss = NegativeSamplingLoss(w_out,corpus)
              layers = self.in_layers+[self.ns_loss]
              
              # 기울기, 가중치 한 곳에 모으기
              self.params=[]
              self.grads=[]
              for layer in layers:
                  self.params+=layer.params
                  self.grads+=layer.grads
              # 단어 벡터 표현 저장
              self.word_vecs = w_in
          def forward(self,contexts,target):
                h=0
                for i,layer in enumerate(self.in_layers):
                h+=layer.forward(contexts[:,i])
                h=h/len(self.in_layers)
                loss = self.ns_loss.forward(h,target)
                return loss


           def backward(self,dout=1):
                dout=self.ns_loss.backward(dout)
                dout*=len(self.in_layers)
                for layer in self.in_layers:
                    layer.backward(dout)
                return None
 ```
 
 ## 개선된 CBOW로 ptb데이터를 학습

```python

from trainer import Trainer
from optimizer import Adam
from dataset import ptb

window_size=5
hidden_size=100
batch_size=100
max_epoch=10
corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)
contexts, target = create_contexts_target(corpus,window_size)
model = CBOW2(vocab_size,hidden_size, window_size,corpus)
optimzer = Adam()
trainer = Trainer(model, optimzer)
trainer.fit(contexts, target, max_epoch, batch_size)
trainer.plot()

 ```
                
