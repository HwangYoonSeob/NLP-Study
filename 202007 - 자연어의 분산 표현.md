# CHAPTER 2

## 자연어 처리의 한 방법 → 시소러스
- 유의어 사전
- 단어 사이의 상하위 관계를 나타냄(단어 네트워크)
- 가장 유명한 시소러스 → WordNet

## 문제점
- 시대 변화에 대응 어렵다
- 방대한 양의 단어를 정의해야 하므로 인적 비용이 든다
- 단어의 미묘한 차이를 표현할 수 없다.
- 따라서 통계 기반 기법, 신경망을 이용한 ‘추론 기반 기법’을 사용

## 통계 기반 기법
- 말뭉치를 사용하여 자연어 처리

- <전처리>
  - split 분할
  - corpus, word_to_id, id_to_word 만들기
  
- 단어의 의미를 정확하게 할 수 있는 벡터 표현: 단어의 분산 표현
- 맥락-주목하는 단어 주변에 놓인 단어(맥락의 크기 = 윈도우 크기)
- 동시 발생 행렬- 어떤 단어에 주목하였을 때 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세
어 집계하는 방법 = python으로 구현하는 방법, 여기서 단어를 벡터로 표현할 수 있게 만듦
벡터 간 유사도- 코사인 유사도 (분자:벡터 내적, 분모:벡터norm) → 두 벡터가 가리키는 방향이
얼마나 유사한지? -1 ~ +1

- 코사인 유사도를 기준으로 most_similiar 함수로 유사 단어 벡터 찾기
- 동시 발생 횟수는 좋은 지표가 되지 않을 수 있다. (‘the’, ‘car’는 동시 등장 횟수가 크므로 관련성
이 높게 나오는 문제)
- 점별상호정보량(PMI) : C(X,Y)*N/C(X)C(Y) , 각 단어의 단독 출연 횟수를 고려 -PPMI(양의 상호정
보량) 함수 구현

- 말뭉치의 어휘 수가 증가함에 따라 행렬 사이즈가 너무 커지는 문제 발생 → 벡터의 차원 감소
필요: SVD 특잇값 분해, 특히 희소벡터의 차원 감소 목적
- SVD: X=USVT 밀집벡터 U를 2차원 벡터로 표현 후 그리기 , X가 PPMI행렬?
- 차원감소- Page 105, 차원을 얼마나 감소시키는지에 대한 정확한 python 코드 필요 x ?
- PTB 데이터셋 – 전체 복습을 위한 코드 구현해보기 (해당 주차 목표)

## 실습

``` python
from dataset import ptb

corpus, word_to_id, id_to_word = ptb.load_data('train')
text='You say goodbye and I say hello .'
text=text.lower()
words=text.split()
words
['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']

corpus=[]
for i in words:
 corpus.append(word_to_id[i])

word_to_id={}
a=0
for i in words:
 if i not in word_to_id:
 word_to_id[i]=a
 a+=1

id_to_word={}
a=0
for i in words:
 if i not in list(id_to_word.values()):
 id_to_word[a]=i
 a+=1

import numpy as np


def co_matrix_create(corpus,window_size):

 vocab_size=max(corpus)+1
 co_matrix=np.zeros((vocab_size,vocab_size))

 for i,j in enumerate(corpus):
 left_window=i-window_size
 right_window=i+window_size

 for k in range(left_window,right_window+1):
 if(k>=0 and k<len(corpus) and i!=k):
 co_matrix[j][corpus[k]]+=1

 return co_matrix
 
def cos_similarity(a,b):
 return np.dot(a,b)/(np.sqrt(sum(a**2))*np.sqrt(sum(b**2)))


def most_similar(word,corpus,window_size,top=5):

 a=co_matrix_create(corpus,window_size)

 abc={}
 for i in range(len(a)):
 if(id_to_word[i]!=word):

abc[id_to_word[i]]=cos_similarity(a[word_to_id[word]],a[i])

 abcd=sorted(abc.items(),key=lambda item:
item[1],reverse=True)

 print(word,'의','상위',top,'개의 most similar 단어는')
 print('')

 for i in range(5):
 print(abcd[i][0],':',abcd[i][1])


def ppmi(corpus,window_size,eps=1e-8):

 vocab_size=max(corpus)+1
 ppmi=np.zeros((vocab_size,vocab_size))

 for i,j in enumerate(corpus):
 left_window=i-window_size
 right_window=i+window_size

 for k in range(left_window,right_window+1):
 if(k>=0 and k<len(corpus) and i!=k):
 ppmi[j][corpus[k]]+=1
 a=np.sum(ppmi)
 b=np.sum(ppmi,axis=0)
 for i in range(len(ppmi)):
 for j in range(len(ppmi)):

ppmi[i][j]=round(max(np.log2(ppmi[i][j]*a/b[i]/b[j]+eps),0),3)

 return ppmi

w=ppmi(corpus,1)

u,s,v=np.linalg.svd(w)
u[0]

array([ 3.40880407e-01, 4.44089210e-16, 1.20638083e-01, -
2.44249065e-15,
 9.32334168e-01, 2.22044605e-16, 1.33161671e-16])
