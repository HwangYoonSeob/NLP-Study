# 순환 신경망(RNN) 구현

```python

import numpy as np

class RNN:
     def __init__(self,Wx,Wh,b):
        self.params=[Wx,Wh,b]
        self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]
        self.cache=None #역전파 진행시 중간 데이터를 담을 변수
      
      def forward(self,x,h_prev):
          Wx,Wh,b = self.parmas
          t=np.matmul(h_prev,Wh)+np.matmul(x,Wx)+b
          h_next=np.tanh(t)
          self.cache=(x,h_prev,h_next)
          return h_next

      def backward(self,dh_next):
          Wx,Wh,b = self.cache
          x,h_prev,h_next = self.cache
          dt=dh_next*(1-h_next**2) # tanh 역전파
          db=np.sum(dt,axis=0) # repeat 노드 역전파
          dWh=np.matmul(h_prev.T,dt) # matmul 노드 역전파 ( h_prev x Wh )
          dh_prev=np.matmul(dt,Wh.T) # matmul 노드 역전파( h_prev x Wh )
          dWx=np.matmul(x.T,dt) # matmul 노드 역전파( X x Wx )
          dx=np.matmul(dt,Wx.T) # matmul 노드 역전파( X x Wx )
          
          self.grads[0][...]=dWx
          self.grads[1][...]=dWh
          self.grads[2][...]=db
          return dx,dh_prev
          
class TimeRNN:
      def __init__(self,Wx,Wh,b,stateful=False):
          self.params=[Wx,Wh,b]
          self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]
          self.layer=None
          self.h, self.dh = None, None
          self.stateful=stateful
          
      def set_state(self,h):
          self.h = h
          
      def reset_state(self):
          self.h = None
          
      def forward(self,xs):
          Wx,Wh,b = self.params
          N,T,D = xs.shape
          D,H = Wx.shape
          self.layers=[]
          hs=np.empty((N,T,H), dtype='f')
          if not self.stateful or self.h is None:
              self.h=np.zeros((N,H), dtype='f')
          for t in range(T):
              layer=RNN(*self.params)
              self.h=layer.forward(xs[:,t,:], self.h)
              hs[:,t:,]=self.h
              self.layers.append(layer)
          return hs
          
       def backward(self, dhs):
            Wx,Wh,b = self.params
            N,T,H = dhs.shape
            D,H = Wx.shape
            dxs=np.empty((N,T,D),dtype='f')
            dh=0
            grads=[0,0,0]
            for t in reverse(range(T)):
                layer=self.layers[t]
                dx,dh = layers.backward(dhs[:,t,:]+dh)
                dxs[:,t,:]=dx
            for i, grad in enumerate(layer.grads):
                  grads[i]+=grad
                  
            for i, grad in enumerate(grads):
                self.grads[i][...]=grad
            self.dh=dh
            return dxs
            
class SimpleRnnlm:
      def __init__(self,vocab_size,wordvec_size,hidden_size):
            V,D,H = vocab_size, wordvec_size, hidden_size
            rn=np.random.randn
            embed_W = (rn(V,D) /100).astype('f')
            rnn_Wx = (rn(D,H) / np.sqrt(D)).astype('f')
            rnn_Wh = (rn(H,H) / np.sqrt(H)).astype('f')
            rnn_b = np.zeros(H).astype('f')
            affine_W = (rn(H,V)/np.sqrt(V)).astype('f')
            affine_b = np.zeros(V).astype('f')
            self.layers=[TimeEmbedding(embed_W), TimeRNN(rnn_Wx,rnn_Wh,rnn_b,True), TimeAffine(affine_W
 ,affine_b)]
            self.loss_layer = TimeSoftmaxWithLoss()
            self.rnn_layer = self.layers[1]
            self.params, self.grads= [], []
            for layer in self.layers:
                self.params+=layer.params
                self.grads+=layer.grads
       def forward(self,xs,ts):
            for layer in self.layers:
                xs=layer.forward(xs)
            loss = self.loss_layer.forward(xs,ts)
            return loss            
       def backward(self,dout=1):
            dout= self.loss_layer.backward(dout)
            for layer in reversed(self.layers):
                dout=layer.backward(dout)
            return dout
        def reset_state(self):
            self.rnn_layer.reset_state()       
```

